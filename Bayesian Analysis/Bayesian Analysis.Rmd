---
title: "Bayesian Analysis"
author: "Michael Belias"
date: "April 20, 2017"
output:
  pdf_document:
    highlight: monochrome
    includes:
      in_header: headerlogo.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    css: APAStyle.css
    fig_caption: yes
    highlight: espresso
    includes:
      in_header: headerlogo.tex
    theme: null
  word_document:
    toc: yes
    toc_depth: '3'
fontsize: 12pt
csl: Harvard2.csl
bibliography: citations.bib
---


\fontfamily{ptm}
\fontsize{14}{22}
\selectfont

```{r global_options, include=FALSE }
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      echo=FALSE, warning=FALSE, message=FALSE,comment = "")
```

\newpage

# Introduction

According to the Oxford dictionary, statistics is a branch of mathematics dealing with the collection, analysis, interpretation, presentation, and organization of data. Data may be of any applied science field such as medical, finance, social, physics etc and they can be separated into 2 types quantitative and qualitative.  


The typical steps of a statistical analysis are seven in general : 

  1) Define the problem 
  
  2) Data collection and manipulation 
  
  3) Explore the data 
  
  4) Using the above three decide the model that will be used
  
  5) Fit the model
  
  6) Check the model and develop if necessary
  
  7) Make the final model and infere
  
 
The above step are not distinct and in some cases there are overlaps and more steps nested. The same principles can be applied in the Bayesian Framework too.

In this tutorial we will learn:
  
  * The bayesian intuition
  * Fit the bayesian methods in simple popular statistical approaches such as:
      + (M)ANOVA
      + Linear Regression
      + Poisson regression
  * Multi-lelel modeling
      + Hierarchical models
      + Meta analysis


\newpage

# Common probability distributions

## Discrete distributions

### Uniform
The uniform distribution is the simplest discrete probability distribution. It assigns equal probability to N different outcomes, usually represented with numbers 1,2,....,N .

X ~ Uniform(N)

$P(X = x|N) = 1/N$ for x = 1,2,...N

$E[X]= \frac{N+1}{2}$

$Var[X]= \frac{N^2+1}{12}$

One common example is the outcome of throwing a fair six-sided die where N=6. 


```{r,echo=FALSE}
set.seed(51)
library(ggplot2)
library(knitr)
x= sample(1:6, size = 10000, replace = T)
barplot(table(x), col = "darkgreen" , axes = F)
```


\newpage


### Bernoulli

The Bernoulli distribution is used for binary outcomes, such as 0 and 1. 
It has one parameter p, which is the probability of "success" frequently geting 1 (or any value we set).
X ~ Bern(p)

$P(X = x | p) = p^x(1-p)^{1-x}$ for x = 0, 1

E[X] = p

Var[X] = p(1-p)

One common example is the outcome of flipping a fair coin (p = 0.5)

\newpage

### Binomial

The binomial distribution counts the number of "successes" in n independent Bernoulli trials
(each with the same probability of success). Thus if $X_1 , X_2 ,..., X_n$ are independent Bernoulli(p) random variables, then Y = $\sum^n_{i=1} X_i$ is binomial distributed.

Y ~Binom(n, p)

P(Y= y|n,p) = $\binom{n}{y} {p^y}(1-p)^{(n-y)}$ , for y = 0,1, ..., n

E[Y] =np

Var[Y]= np(1-p)

where $\binom{n}{y}= \frac{n!}{y!(n-y)!}$ .

\newpage


### Poisson

The Poisson distribution is used for counts, and arises in a variety of situations. The parameter $\lambda$ > 0 is the rate at which we expect to observe the thing we are counting.

X~Pois($\lambda$)

$P(X= x|\lambda) = \frac{\lambda^xexp(-\lambda)}{x!}$

E[X] = $\lambda$

Var[X] = $\lambda$

A Poisson process is a process wherein events occur on average at rate $\lambda$, events occur one at a time, and events occur independently of each other.

Example: 

Significant earthquakes occur in the Western United States approximately following a Poisson process with rate of two earthquakes per week. What is the probability
there will be at least 3 earthquakes in the next two weeks? Answer: the rate per two weeks is
2*2 = 4, so let X ~Pois(4) and we want to know 
$P(X \geq 3)=1-(X\leq 2) =1- P(X = 0) - P( X = 1 ) - P( X = 2) =1 - e^{-4} _ 4e^{-4} -\frac{4^2 e^{-4}}{2} = 1 - 13e^{-4} = 0.762$



Note that 0! = 1  by definition.
\newpage

### Geometric


The geometric distribution is the number of failures before obtaining the first success, i.e., the number of Bernoulli failures until a success is observed, such as the first head when flipping a coin. It takes values on the positive integers starting with 0 (alternatively, we could count total trials until first success, in which case we would start with 1).

X~Geo(p)

$P(X=x|p) = p(1-p)^x$ , for x=1,2,...

$E[X] = \frac{1-p}{p}$

If the probability of getting a success is p, then the expected number of trials until the first success is 1=p and the expected number of failures until the first success is (1 - p)=p.


\newpage

### Negative Binomial
The negative binomial distribution extends the geometric distribution to model the number of failures before achieving the rth success. It takes values on the positive integers starting
with 0.


Y~NegBinom(r,p)

P(Y= y|n,p) = $\binom{r + y - 1}{y} {p^r}(1-p)^{(y)}$ for y=1,2,...

E[Y] = $\frac{r(1-p)}{p}$

Var[Y] = $\frac{r(1-p)}{p^2}$

Note that the geometric distribution is a special case of the negative binomial distribution where r = 1.
Because 0 < p < 1, we have E[Y] < Var[Y]. This makes the negative binomial a popular alternative to the Poisson when modeling counts with high variance (recall, that the mean equals the variance for Poisson distributed variables). 

\newpage


### Multinomial

Another generalization of the Bernoulli and the binomial is the multinomial distribution, which is like a binomial when there are more than two possible outcomes. Suppose we have n trials and there are k different possible outcomes which occur with probabilities $p_1,p_2,... p_k$. 
For example, we are rolling a six-sided die that might be loaded so that the sides are not equally likely, then n is the total number of rolls, k = 6, p1 is the probability of rolling a one, and we denote by $x_1,x_2, ...,x_6$ a possible outcome for the number of times we observe rolls of
each of one through six, where $\sum_{i=1}^{6}x_i = n$ and $\sum_{i=1}^{6}p_i = 1$

\newpage


# Bayesian Theory
## History

  Bayesian statistics are based on the homonymous Bayes' theorem or rule, invented by Thomas Bayes, which was a british reverend the 1740s . His primary field of studying was theology but Bayes was also "amateur" mathematician. He was influenced by David Hume a philosopher teacher while his studies in Edinburgh proposing that we can only rely on what we learn from experience. The probabilities as a mathematical field these days where just emerging being able to solve simple problems like *what is the probability of observing an effect given a cause?* but not the inverse P(cause | effect). Bayes gave a simple example of tossing balls on a table and recording where they stop (to the left or to the right side of the table), noting that the more balls are thrown, the better we may infere if the ball-tosing is bias to a side. This is nowadays called a learning process and although it was a remarkable finding Bayes forgot it in a drawer (!) until his death.Richard Price found it and after studing his papers for 2 years and making some corrections he finally published **An Essay toward solving a Problem in the Doctrine of Chances”. 1763**. 

Still the theorem was just an example not having the final form of today and even after this publication no-one really continued the development except of Laplace, who was trying to solve an astronomical problem , studied Price's paper developed a first version of what we now call Bayes theorem. The reception of Laplace's proposal  was slightly hostile due to the inherent challenges such as the equal prior probabilities, being subjective and the serious technical computational problems in practice, which is still a great issue .

\newpage

## Bayes theorem


Bayes theorem is calculating the probability event given prior knowledge of conditions that might be related to the event. Bayes' theorem is stated mathematically as the following equation [@Efron1177] :

${P(A\mid B)={\frac {P(B\mid A)\,P(A)}{P(B)}}}$ 

${P(A\mid B)={\frac {P(B\mid A)\,P(A)}{P(B)}}}$

where A and B are events and P(B) $\neq$ 0.

* P(A) and P(B) are the probabilities of observing A and B without regard to each other.
* P(A | B), a conditional probability, is the probability of observing event A given that B is true.
* P(B | A) is the probability of observing event B given that A is true.

This is the basis of Bayesian inference which is a particular approach to statistical inference, with it's own interpretation and  When applied, the probabilities involved in Bayes’ theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.

\newpage

# Monte Carlo estimation

## Simulation and CLT

Before we learn how to simulate from complicated posterior distributions, let’s review some of the basics of Monte Carlo estimation. Monte Carlo estimation refers to simulating hypothetical draws from a probability distribution in order to calculate important quantities. These quantities might include the mean, the variance, the probability of some event, or quantiles of the distribution. All of these calculations involve integration, which except for the simplest distributions, can be very difficult or impossible.

Suppose we have a random variable $\hat \theta$ that follows a Gamma(a,b). Let’s say a=2 and b=1/3 , where b is the rate parameter. To calculate the mean of this distribution, we would need to compute the following integral

$$\text{E}(\theta) = \int_0^\infty \theta f(\theta) d\theta = \int_0^\infty \theta \frac{b^a}{\Gamma(a)}\theta^{a-1}e^{-b\theta} d\theta \,$$

It is possible to compute this integral, and the answer is a/b (6 in this case). However, we could verify this answer through Monte Carlo  estimation. To do so, we would simulate a large number of draws (call them $\theta_i^*$ for i = 1,2,...,m) from this gamma distribution and calculate their sample mean. Why can we do this? Recall from the previous course that if we have a random sample from a distribution, the average of those samples converges in probability to the true mean of that distribution by the Law of Large Numbers. Furthermore, by the Central Limit Theorem (CLT), this sample mean $\bar \theta^* = \frac{1}{m} \sum_{i=1}^{m} \theta_i^*$ approximately follows a normal distribution with mean E($\theta$) and variance Var($\theta$)/m. The theoretical variance of $\theta$ is the following integral: $$\text{Var}(\theta) = \int_0^\infty (\theta-E(\theta))^2 f(\theta) d\theta \,$$

Just like we did with the mean, we could approximate this variance with the sample variance $\frac{1}{m}\sum_{i=1}^m (\theta_i^* - \bar{\theta}^*)^2$


##Calculating probabilities

This method can be used to calculate many different integrals. Say h($\theta$) is any function and we want to calculate $\int h(\theta) p(\theta) d\theta$ This integral is precisely what is meant by E(h($\theta$)), so we can conveniently approximate it by taking the sample mean of h($\theta_i^*$). That is, we apply the function hh to each simulated sample from the distribution, and take the average of all the results. 

One extremely useful example of an hh function is is the indicator $I_A(\theta)$ where A is some logical condition about the value of $\theta$. To demonstrate, suppose $h(\theta) = I_{[\theta<5]}(\theta)$ , which will give a 1 if $\theta<5$ and 0 otherwise.

The E(h($\theta$)) = $\int_0^\infty I_{[\theta<5]}(\theta) p(\theta) d\theta = \int_0^5 1 \cdot p(\theta) d\theta + \int_5^\infty 0 \cdot p(\theta) d\theta = P(\theta < 5) \,$ . 
This means we can approximate the probability that $\theta < 5$ by drawing many samples $\theta_i^*$ , and approximating this integral with  $\frac{1}{m} \sum_{i=1}^m I_{\theta^* < 5} (\theta_i^*)$. This expression is simply counting how many of those samples come out to be less than 55, and dividing by the total number of simulated samples. So simple!

Likewise, we can approximate quantiles of a distribution. If we are looking for the value $\zeta$ such that P($\theta$ < z) = 0.9 , we simply arrange the samples $\theta_i^*$ in ascending order and find the smallest drawn value that is greater than 90% of the others.

## Monte Carlo error

How good is an approximation by Monte Carlo sampling? Again we can turn to the CLT, which tells us that the variance of our estimate is controlled in part by m . For a better estimate, we want larger m . 

For example, if we seek E($\theta$), then the sample mean $\bar \theta^*$ approximately follows a normal distribution with mean  E($\theta$) and variance  Var($\theta$)/m. The variance tells us how far our estimate might be from the true value. One way to approximate  Var($\theta$) is to replace it with the sample variance. The standard deviation of our Monte Carlo estimate is the square root of that, or the sample standard deviation divided by $\sqrt m$. If m is large, it is reasonable to assume that the true value will likely be within about two standard deviations of your Monte Carlo estimate.

## Marginalization

We can also obtain Monte Carlo samples from hierarchical models. As a simple example, let’s consider a binomial random variable where $y \mid \phi \sim \text{Bin}(10, \phi)$,  and further suppose $\phi$ is random (as if it had a prior) and is distributed beta $\phi ~ Beta(2,2)$. Given any hierarchical model, we can always write the joint distribution of y and $\phi$ as $p(y, \phi) = p(y \mid \phi)p(\phi)$ using the chain rule of probability. To simulate from this joint distribution, repeat these steps for a large number m : 

* Simulate $\phi_i^*$ rom its Beta(2,2)distribution
* Given the drawn $\phi_i^*$ ,simulate $y_i^*$ from $\text{Bin}(10, \phi_i^*)$ 

This will produce m independent pairs of $(y^*, \phi^*)_i$ drawn from their joint distribution. One major advantage of Monte Carlo simulation is that marginalizing is easy. Calculating the marginal distribution of y, $p(y) = \int_0^1 p(y, \phi) d\phi$ might be challenging. But if we have draws from the joint distribution, we can just discard the $\phi_i^*$ raws and use the $y_i^*$as samples from their marginal distribution. This is also called the prior predictive distribution introduced in the previous course. 

In the next segment, we will demonstrate some of these principles. Remember, we do not yet know how to sample from the complicated posterior distributions introduced in the previous lesson. But once we learn that, we will be able to use the principles from this lesson to make approximate inferences from those posterior distributions.

\newpage

# Markov chains

Definition
If we have a sequence of random variables $X_1,X_2,...,X_n$ where the indices *1,2,…,n* represent successive points in time, we can use the chain rule of probability to calculate the probability of the entire sequence:

$p(X_{t+1} | X_t, X_{t-1}, \ldots, X_2, X_1 ) = p(X_{t+1} | X_t) \,$

Markov chains simplify this expression by using the *Markov assumption*. The assumption is that given the entire past history, the probability distribution for the random variable at the next time step only depends on the current variable. Mathematically, the assumption is written like this:

$p(X_{t+1} | X_t, X_{t-1}, \ldots, X_2, X_1 ) = p(X_{t+1} | X_t) \,$

for all *t=2,…,n*. Under this assumption, we can write the first expression as
$p(X_1, X_2, \ldots X_n) = p(X_1) \cdot p(X_2 | X_1) \cdot p(X_3 | X_2) \cdot p(X_4 | X_3) \cdot \ldots \cdot p(X_n | X_{n-1}) \, ,$


which is much simpler than the original. It consists of an initial distribution for the first variable, $P(X_1)$ , and **n - 1** transition probabilities. We usually make one more assumption: that the transition probabilities do not change with time. Hence, the transition from time tt to time **t+1** depends only on the value of $X_t$.

## Examples of Markov chains
### Discrete Markov chain

Suppose you have a secret number (make it an integer) between 1 and 5. We will call it your initial number at *step 1*. Now for each time step, your secret number will change according to the following rules:

1. Flip a coin.
2. 
  + If the coin turns up heads, then increase your secret number by one (5 increases to 1).
  + If the coin turns up tails, then decrease your secret number by one (1 decreases to 5).

3. Repeat *n** times, and record the evolving history of your secret number.


Before the experiment, we can think of the sequence of secret numbers as a sequence of random variables, each taking on a value in **{1,2,3,4,5}**. Assume that the coin is fair, so that with each flip, the probability of heads and tails are both 0.5.

Does this game qualify as a true Markov chain? Suppose your secret number is currently 4 and that the history of your secret numbers is **(2,1,2,3)**. What is the probability that on the next step, your secret number will be 5? What about the other four possibilities? Because of the rules of this game, the probability of the next transition will depend only on the fact that your current number is 4. The numbers further back in your history are irrelevant, so this is a Markov chain.

This is an example of a discrete Markov chain, where the possible values of the random variables come from a discrete set. Those possible values (secret numbers in this example) are called states of the chain. The states are usually numbers, as in this example, but they can represent anything. In one common example, the states describe the weather on a particular day, which could be labeled as 1-fair, 2-poor.

### Random walk (continuous)

Now let’s look at a continuous example of a Markov chain. Say Xt=0Xt=0 and we have the following transition model: $p(X_{t+1} | X_t=x_t) = \text{N}(x_t, 1),$. That is, the probability distribution for the next state is Normal with variance 1 and mean equal to the current state. This is often referred to as a “random walk.” Clearly, it is a Markov chain because the transition to the next state  $X_{t+1}$only depends on the current state $X_t$.

**R-code**

```{r}
set.seed(34)

n = 100
x = numeric(n)

for (i in 2:n) {
  x[i] = rnorm(1, mean=x[i-1], sd=1.0)
}

plot.ts(x)
```


### Transition matrix
Let’s return to our example of the discrete Markov chain. If we assume that transition probabilities do not change with time, then there are a total of $5^2 = 25$ potential transition probabilities. Potential transition probabilities would be from *State 1* to *State 2*, *State 1* to *State 3*, and so forth. These transition probabilities can be arranged into a matrix *Q*:

$$Q = 
\begin{pmatrix}
0 & .5 & 0 & 0 & .5 \\
.5 & 0 & .5 & 0 & 0 \\
0 & .5 & 0 & .5 & 0 \\
0 & 0 & .5 & 0 & .5 \\
.5 & 0 & 0 & .5 & 0 \\
\end{pmatrix}$$


where the transitions from *State 1*are in the first row, the transitions from *State 2* are in the second row, etc. For example, the probability $p(X_{t+1} = 5 \mid X_t = 4)$ can be found in the fourth row, fifth column.

The transition matrix is especially useful if we want to find the probabilities associated with multiple steps of the chain. For example, we might want to know $p(X_{t+2}=3 \mid X_t=1)$ , the probability of your secret number being 3 two steps from now, given that your number is currently 1. We can calculate this as $\sum_{k=1}^5 p(X_{t+2}=3 \mid X_{t+1}=k) \cdot p(X_{t+1}=k \mid X_t=1)$ , which conveniently is found in the first row and third column of $Q^2$.


**R-code**

```{r}
Q = matrix(c(0.0, 0.5, 0.0, 0.0, 0.5,
             0.5, 0.0, 0.5, 0.0, 0.0,
             0.0, 0.5, 0.0, 0.5, 0.0,
             0.0, 0.0, 0.5, 0.0, 0.5,
             0.5, 0.0, 0.0, 0.5, 0.0), 
           nrow=5, byrow=TRUE)

Q %*% Q # Matrix multiplication in R. This is Q^2.
```

### Stationary distribution

Suppose we want to know the probability distribution of the your secret number in the distant future, say $p(X_{t+h} | X_t)$ where *h* is a large number. Let’s calculate this for a few different values of *h*.

```{r}
Q5 = Q %*% Q %*% Q %*% Q %*% Q # h=5 steps in the future
round(Q5, 3)
```

```{r}
Q10 = Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q # h=10 steps in the future
round(Q10, 3)
```

```{r}
Q30 = Q
for (i in 2:30) {
  Q30 = Q30 %*% Q
}
round(Q30, 3) # h=30 steps in the future
```

Notice that as the future horizon gets more distant, the transition distributions appear to converge. The state you are currently in becomes less important in determining the more distant future. If we let hh get really large, and take it to the limit, all the rows of the long-range transition matrix will become equal to *(.2,.2,.2,.2,.2)*. That is, if you run the Markov chain for a very long time, the probability that you will end up in any particular state is *1/5=.2* for each of the five states. These long-range probabilities are equal to what is called the stationary distribution of the Markov chain.

The stationary distribution of a chain is the initial state distribution for which performing a transition will not change the probability of ending up in any given state. That is,


```{r}
c(0.2, 0.2, 0.2, 0.2, 0.2) %*% Q
```

One consequence of this property is that once a chain reaches its stationary distribution, the stationary distribution will remain the distribution of the states thereafter.

We can also demonstrate the stationary distribution by simulating a long chain from this example.

```{r}
n = 5000
x = numeric(n)
x[1] = 1 # fix the state as 1 for time 1
for (i in 2:n) {
  x[i] = sample.int(5, size=1, prob=Q[x[i-1],]) # draw the next state from the intergers 1 to 5 with probabilities from the transition matrix Q, based on the previous value of X.
}
```

Now that we have simulated the chain, let’s look at the distribution of visits to the five states.


```{r}
table(x) / n
```

The overall distribution of the visits to the states is approximately equal to the stationary distribution.

As we have just seen, if you simulate a Markov chain for many iterations, the samples can be used as a Monte Carlo sample from the stationary distribution. This is exactly how we are going to use Markov chains for Bayesian inference. In order to simulate from a complicated posterior distribution, we will set up and run a Markov chain whose stationary distribution is the posterior distribution.

It is important to note that the stationary distribution doesn’t always exist for any given Markov chain. The Markov chain must have certain properties, which we won’t discuss here. However, the Markov chain algorithms we’ll use in future lessons for Monte Carlo estimation are guaranteed to produce stationary distributions.

### Continuous example


The continuous random walk example we gave earlier does not have a stationary distribution. However, we can modify it so that it does have a stationary distribution.

Let the transition distribution be $p(X_{t+1} | X_t=x_t) = \text{N}(\phi x_t, 1)$ where *-1<$\phi$<1* .That is, the probability distribution for the next state is Normal with variance 1 and mean equal to $\phi$ times the current state. As long as $\phi$ is between *-1 *and* 1*, then the stationary distribution will exist for this model.

Let’s simulate this chain for $\phi = -0.6$.

```{r}
set.seed(38)

n = 1500
x = numeric(n)
phi = -0.6

for (i in 2:n) {
  x[i] = rnorm(1, mean=phi*x[i-1], sd=1.0)
}

plot.ts(x)
```

The theoretical stationary distribution for this chain is normal with mean 0 and variance 1/(1-$\phi^2$) which in our example approximately equals *1.562*. Let’s look at a histogram of our chain and compare that with the theoretical stationary distribution.


```{r,echo=FALSE}
hist(x, freq=FALSE)
curve(dnorm(x, mean=0.0, sd=sqrt(1.0/(1.0-phi^2))), col="red", add=TRUE)
legend("topright", legend="theoretical stationary\ndistribution", col="red", lty=1, bty="n")
```

It appears that the chain has reached the stationary distribution. Therefore, we could treat this simulation from the chain like a Monte Carlo sample from the stationary distribution, a normal with mean *0* and variance *1.562*.

Because most posterior distributions we will look at are continuous, our Monte Carlo simulations with Markov chains will be similar to this example.


\newpage


## Monte Carlo Example 

\newpage

# Metropolis-Hastings

Metropolis-Hastings is an algorithm that allows us to sample from a generic probability distribution (which we will call the target distribution), even if we do not know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution. It consists of picking an arbitrary starting value, and iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample.

Let’s say we wish to produce samples from a target distribution $p(\theta) \propto g(\theta)$ where we don’t know the normalizing constant (since $\int g(\theta) d\theta$ is hard or impossible to compute), so we only have $g(\theta)$ to work with. The Metropolis-Hastings algorithm proceeds as follows.

1. Select an initial value $\theta_0$ .
2. For i=1,…,m  repeat the following steps:

* Draw a candidate sample $\theta^*$ from a proposal distribution $q(\theta^* \mid \theta_{i-1})$ (more on this later).
*Compute the ratio $\alpha = \frac{g(\theta^*) / q(\theta^* \mid \theta_{i-1}) }{g(\theta_{i-1}) / q(\theta_{i-1} \mid \theta^*)} = \frac{g(\theta^*)q(\theta_{i-1} \mid \theta^*)}{g(\theta_{i-1})q(\theta^* \mid \theta_{i-1})} \,$.

If $\alpha \ge 1$, then set $\theta_i = \theta^*$ 
If $\alpha < 1$, then set $\theta_i = \theta^*$ with probability $\alpha$, or $\theta_i = \theta_{i-1}$ with probability 1-$\alpha$.

Steps 2b and 2c act as a correction since the proposal distribution is not the target distribution. At each step in the chain, we draw a candidate and decide whether to “move” the chain there or remain where we are. If the proposed move to the candidate is “advantageous,” ($\alpha \ge 1$) we “move” there and if it is not “advantageous,” we still might move there, but only with probability $\alpha$. Since our decision to “move” to the candidate only depends on where the chain currently is, this is a Markov chain.

##Proposal distribution
One careful choice we must make is the candidate generating distribution$q(\theta^* \mid \theta_{i-1})$ It may or may not depend on the previous iteration’s value of $\theta$.  One example where it doesn’t depend on the previous value would be if $q(\theta)$is always the same distribution. If we use this option, $q(\theta)$ should be as similar as possible to $p(\theta)$.

Another popular option, one that does depend on the previous iteration, is Random-Walk Metropolis-Hastings. Here, the proposal distribution is centered on $\theta_{i-1}$. For instance, it might be a normal distribution with mean $\theta_{i-1}$. Because the normal distribution is symmetric, this example comes with another advantage: $q(\theta^* \mid \theta_{i-1}) = q(\theta_{i-1} \mid \theta^*)$, causing it to cancel out when we calculate $\alpha$. Thus, in Random-Walk Metropolis-Hastings where the candidate is drawn from a normal with mean $\theta_{i-1}$ and constant variance, the acceptance ratio is $\alpha = \frac{g(\theta^*) }{g(\theta_{i-1})}$.

## Acceptance rate
Clearly, not all candidate draws are accepted, so our Markov chain sometimes “stays” where it is, possibly for many iterations. How often you want the chain to accept candidates depends on the type of algorithm you use. If you approximate $p(\theta)$ with $q(\theta^*)$and always draw candidates from that, accepting candidates often is good; it means $q(\theta^*)$is approximating $p(\theta)$ well. However, you still may want qq to have a larger variance than pp and see some rejection of candidates as an assurance that qq is covering the space well.

As we will see in coming examples, a high acceptance rate for the Random-Walk Metropolis-Hastings sampler is not a good thing. If the random walk is taking too small of steps, it will accept often, but will take a very long time to fully explore the posterior. If the random walk is taking too large of steps, many of its proposals will have low probability and the acceptance rate will be low, wasting many draws. Ideally, a random walk sampler should accept somewhere between 23% and 50% of the candidates proposed.

In the next segment, we will see a demonstration of this algorithm used in a discrete case, where we can show mathematically that the Markov chain converges to the target distribution. In the following segment, we will demonstrate coding a Random-Walk Metropolis-Hastings algorithm in  R to solve one of the problems from the end of Lesson 2.

## Random walk with normal likelihood, t prior

Recall the model from the last segment of Lesson 2 where the data are the percent change in total personnel from last year to this year for *n=10* companies. We used a normal likelihood with known variance and *t* distribution for the prior on the unknown mean. Suppose the values are *y=(1.2,1.4,-0.5,0.3,0.9,2.3,1.0,0.1,1.3,1.9)* . Because this model is not conjugate, the posterior distribution is not in a standard form that we can easily sample. To obtain posterior samples, we will set up a Markov chain whose stationary distribution is this posterior distribution.

Recall that the posterior distribution is: 

$$p(\mu \mid y_1, \ldots, y_n) \propto \frac{\exp[ n ( \bar{y} \mu - \mu^2/2)]}{1 + \mu^2}$$
The posterior distribution on the left is our target distribution and the expression on the right is our g($\mu$).

The first thing we can do in R is write a function to evaluate g($\mu$). Because posterior distributions include likelihoods (the product of many numbers that are potentially small), g($\mu$) might evaluate to such a small number that to the computer, it effectively zero. This will cause a problem when we evaluate the acceptance ratio $\alpha$. To avoid this problem, we can work on the log scale, which will be more numerically stable. Thus, we will write a function to evaluate

$$\log(g(\mu)) = n ( \bar{y} \mu - \mu^2/2) - \log(1 + \mu^2)$$

This function will require three arguments, $\mu$ , $\bar{y}$ and n.

```{r}
lg = function(mu, n, ybar) {
  mu2 = mu^2
  n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)
}
```

Next, let’s write a function to execute the Random-Walk Metropolis-Hastings sampler with normal proposals.

```{r}
mh = function(n, ybar, n_iter, mu_init, cand_sd) {
  ## Random-Walk Metropolis-Hastings algorithm
  
  ## step 1, initialize
  mu_out = numeric(n_iter)
  accpt = 0
  mu_now = mu_init
  lg_now = lg(mu=mu_now, n=n, ybar=ybar)
  
  ## step 2, iterate
  for (i in 1:n_iter) {
    ## step 2a
    mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
    
    ## step 2b
    lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
    lalpha = lg_cand - lg_now # log of acceptance ratio
    alpha = exp(lalpha)
    
    ## step 2c
    u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
    if (u < alpha) { # then accept the candidate
      mu_now = mu_cand
      accpt = accpt + 1 # to keep track of acceptance
      lg_now = lg_cand
    }
    
    ## collect results
    mu_out[i] = mu_now # save this iteration's value of mu
  }
  
  ## return a list of output
  list(mu=mu_out, accpt=accpt/n_iter)
}
```

Now, let’s set up the problem.

```{r}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
```

Finally, we’re ready to run the sampler! Let’s use *m=1000* iterations and proposal standard deviation (which controls the proposal step size) 3.0, and initial value at the prior median 0


```{r,comment=""}
set.seed(43) # set the random seed for reproducibility
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)
str(post)

library("coda")
traceplot(as.mcmc(post$mu))
```

This last plot is called a trace plot. It shows the history of the chain and provides basic feedback about whether the chain has reached its stationary distribution.

It appears our proposal step size was too large (acceptance rate below 23%). Let’s try another.

```{r}
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.05)
post$accpt
traceplot(as.mcmc(post$mu))

post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.9)
post$accpt

traceplot(as.mcmc(post$mu))
```

Hey, that looks pretty good. Just for fun, let’s see what happens if we initialize the chain at some far-off value.

```{r}
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=30.0, cand_sd=0.9)
post$accpt
```

```{r}
traceplot(as.mcmc(post$mu))
```

It took awhile to find the stationary distribution, but it looks like we succeeded! If we discard the first 100 or so values, it appears like the rest of the samples come from the stationary distribution, our posterior distribution! Let’s plot the posterior density against the prior to see how the data updated our belief about $\mu$.

```{r}
post$mu_keep = post$mu[-c(1:100)] # discard the first 200 samples
plot(density(post$mu_keep, adjust=2.0), main="", xlim=c(-1.0, 3.0), xlab=expression(mu)) # plot density estimate of the posterior
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(ybar, 0, pch=19) # sample mean

curve(0.017*exp(lg(mu=x, n=n, ybar=ybar)), from=-1.0, to=3.0, add=TRUE, col="blue") # approximation to the true posterior in blue
```


These results are encouraging, but they are preliminary. We still need to investigate more formally whether our Markov chain has converged to the stationary distribution. We will explore this in a future lesson.

Obtaining posterior samples using the Metropolis-Hastings algorithm can be time-consuming and require some fine-tuning, as we’ve just seen. The good news is that we can rely on software to do most of the work for us. In the next couple of videos, we’ll introduce a program that will make posterior sampling easy.

\newpage

# Gibbs sampling

So far, we have demonstrated MCMC for a single parameter. What if we seek the posterior distribution of multiple parameters, and that posterior distribution does not have a standard form? One option is to perform Metropolis-Hastings (M-H) by sampling candidates for all parameters at once, and accepting or rejecting all of those candidates together. While this is possible, it can get complicated. Another (simpler) option is to sample the parameters one at a time.

As a simple example, suppose we have a joint posterior distribution for two parameters $\theta$ and $\phi$, written $P(\theta,\phi|y) \propto g (\theta,\phi)$ . If we knew the value of $\phi$, then we would just draw a candidate for $\theta$ and use $g(\theta,\phi)$ to compute our Metropolis-Hastings ratio, and possibly accept the candidate. Before moving on to the next iteration, if we don’t know $\phi$, then we can perform a similar update for it. Draw a candidate for $\phi$ using some proposal distribution and again use $g(\theta,\phi)$ to compute our Metropolis-Hastings ratio. Here we pretend we know the value of $\theta$ by substituting its current iteration from the Markov chain. Once we’ve drawn for both $\theta$ and $\phi$, that completes one iteration and we begin the next iteration by drawing a new $\theta$. In other words, we’re just going back and forth, updating the parameters one at a time, plugging the current value of the other parameter into $g(\theta,\phi)$.

This idea of one-at-a-time updates is used in what we call Gibbs sampling, which also produces a stationary Markov chain (whose stationary distribution is the posterior).


## Full conditional distributions

Before describing the full Gibbs sampling algorithm, there’s one more thing we can do. Using the chain rule of probability, we have $p(\theta, \phi \mid y) = p(\theta \mid \phi, y) \cdot p(\phi \mid y)$. notice that the only difference between $p(\theta, \phi \mid y)$ and $p(\theta \mid \phi, y)$ is multiplication by a factor that doesn’t involve $\theta$ . Since the $g(\theta,\phi)$ function above, when viewed as a function of $\theta$ is proportional to both these expressions, we might as well have replaced it with $p(\theta \mid  \phi,y)$ in our update for $\theta$.This distribution $p(\theta \mid \phi, y)$  is called the full conditional distribution for $\theta$. Why use it instead of $g(\theta,\phi)$? In some cases, the full conditional distribution is a standard distribution we know how to sample. If that happens, we no longer need to draw a candidate and decide whether to accept it. In fact, if we treat the full conditional distribution as a candidate proposal distribution, the resulting Metropolis-Hastings acceptance probability becomes exactly 1.

Gibbs samplers require a little more work up front because you need to find the full conditional distribution for each parameter. The good news is that all full conditional distributions have the same starting point: the full joint posterior distribution. Using the example above, we have $p(\theta \mid  \phi,y) \propto p(\theta,\phi|y)$
where we simply now treat $\phi$ as a known number. Likewise, the other full conditional is $p(\phi \mid   \theta ,y) \propto p(\theta,\phi\mid  y)$ where here, we consider $\theta$ to be a known number. We always start with the full posterior distribution. Thus, the process of finding full conditional distributions is the same as finding the posterior distribution of each parameter, pretending that all other parameters are known.

## Gibbs sampler

The idea of Gibbs sampling is that we can update multiple parameters by sampling just one parameter at a time, cycling through all parameters and repeating. To perform the update for one particular parameter, we substitute in the current values of all other parameters.

Here is the algorithm. Suppose we have a joint posterior distribution for two parameters $\theta$ and $\phi$, written $P(\theta,\phi\mid  y)$. If we can find the distribution of each parameter at a time, i.e., $P(\theta \mid   \phi,y)$ and $P(\phi\mid   \theta,y)$, then we can take turns sampling these distributions like so:

1.Using $\phi_{i-1}$draw $\theta_i$ from $P(\theta | \phi= \phi_{i-1} ,y )$ .

2.Using $\theta_i$, draw $\phi_i$   from $P(\phi|\theta=\theta_i,y)$.

Together, steps 1 and 2 complete one cycle of the Gibbs sampler and produce the draw for ($\theta_i , \phi_i$) in one iteration of a MCMC sampler. If there are more than two parameters, we can handle that also. One Gibbs cycle would include an update for each of the parameters.

## Example 
### Normal likelihood, unknown mean and variance

Let’s make an example, where we have normal likelihood with unknown mean and unknown variance. The model is : 
$$ \begin{aligned}
y_i \mid \mu, \sigma^2 &\overset{\text{iid}}{\sim} \text{N} ( \mu, \sigma^2 ), \quad i=1,\ldots,n \\ 
\mu &\sim \text{N}(\mu_0, \sigma_0^2) \\ 
\sigma^2 &\sim \text{IG}(\nu_0, \beta_0)  \
\end{aligned} $$.

We chose a normal prior for $\mu$ because, in the case where $\sigma^2$ is known, the normal is the conjugate prior for $\mu$. Likewise, in the case where $\mu$ is known, the inverse-gamma is the conjugate prior for $\sigma^2$. This will give us convenient full conditional distributions in a Gibbs sampler.

Let’s first work out the form of the full posterior distribution. When we begin analyzing data, the JAGS software will complete this step for us. However, it is extremely valuable to see and understand how this works.


$$ \begin{aligned}
\\
p( \mu, \sigma^2 \mid y_1, y_2, \ldots, y_n ) &\propto
p(y_1, y_2, \ldots, y_n \mid \mu, \sigma^2) p(\mu) p(\sigma^2) = \prod_{i=1}^n \text{N} ( y_i \mid \mu, \sigma^2 ) \times \text{N}( \mu \mid \mu_0, \sigma_0^2) \times \text{IG}(\sigma^2 \mid \nu_0, \beta_0) \\
= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp \left[ -\frac{(y_i - \mu)^2}{2\sigma^2} \right] \times \frac{1}{\sqrt{2\pi\sigma_0^2}} \exp \left[ -\frac{(\mu - \mu_0)^2}{2\sigma_0^2} \right] \times \frac{\beta_0^{\nu_0}}{\Gamma(\nu_0)}(\sigma^2)^{-(\nu_0 + 1)} \exp \left[ -\frac{\beta_0}{\sigma^2} \right] I_{\sigma^2 > 0}(\sigma^2) \\
\propto (\sigma^2)^{-n/2} \exp \left[ -\frac{\sum_{i=1}^n (y_i - \mu)^2}{2\sigma^2} \right] \exp \left[ -\frac{(\mu - \mu_0)^2}{2\sigma_0^2} \right] (\sigma^2)^{-(\nu_0 + 1)} \exp \left[ -\frac{\beta_0}{\sigma^2} \right] I_{\sigma^2 > 0}(\sigma^2)
\end{aligned} $$

From here, it is easy to continue on to find the two full conditional distributions we need. First let’s look at $\mu$, assuming $\sigma^2$ is known (in which case it becomes a constant and is absorbed into the normalizing constant):
$$\begin{aligned}
p(\mu \mid \sigma^2, y_1, \ldots, y_n) &\propto p( \mu, \sigma^2 \mid y_1, \ldots, y_n ) \\
&\propto \exp \left[ -\frac{\sum_{i=1}^n (y_i - \mu)^2}{2\sigma^2} \right] \exp \left[ -\frac{(\mu - \mu_0)^2}{2\sigma_0^2} \right] \\
&\propto \exp \left[ -\frac{1}{2} \left( \frac{ \sum_{i=1}^n (y_i - \mu)^2}{2\sigma^2} + \frac{(\mu - \mu_0)^2}{2\sigma_0^2} \right) \right] \\
&\propto \text{N} \left( \mu \mid \frac{n\bar{y}/\sigma^2 + \mu_0/\sigma_0^2}{n/\sigma^2 + 1/\sigma_0^2}, \, \frac{1}{n/\sigma^2 + 1/\sigma_0^2} \right) \, ,
\end {aligned}$$
which we derived in the supplementary material of the last course. So, given the data and $\sigma^2$, $\mu$ follows this normal distribution.

Now let’s look at $\sigma^2$, assuming $\mu$ is known:
$$\begin{aligned}
p(\sigma^2 \mid \mu, y_1, \ldots, y_n) &\propto p( \mu, \sigma^2 \mid y_1, \ldots, y_n ) \\
&\propto (\sigma^2)^{-n/2} \exp \left[ -\frac{\sum_{i=1}^n (y_i - \mu)^2}{2\sigma^2} \right] (\sigma^2)^{-(\nu_0 + 1)} \exp \left[ -\frac{\beta_0}{\sigma^2} \right] I_{\sigma^2 > 0}(\sigma^2) \\
&\propto (\sigma^2)^{-(\nu_0 + n/2 + 1)} \exp \left[ -\frac{1}{\sigma^2} \left( \beta_0 + \frac{\sum_{i=1}^n (y_i - \mu)^2}{2} \right) \right] I_{\sigma^2 > 0}(\sigma^2) \\
&\propto \text{IG}\left( \sigma^2 \mid \nu_0 + \frac{n}{2}, \, \beta_0 + \frac{\sum_{i=1}^n (y_i - \mu)^2}{2} \right) \, .
\end{aligned}$$

These two distributions provide the basis of a Gibbs sampler to simulate from a Markov chain whose stationary distribution is the full posterior of both $\mu$ and $\sigma^2$. We simply alternate draws between these two parameters, using the most recent draw of one parameter to update the other.

We will do this in R in the next segment.

#### Gibbs sampler in R

To implement the Gibbs sampler we just described, let’s return to our running example where the data are the percent change in total personnel from last year to this year for *n=10* companies. We’ll still use a normal likelihood, but now we’ll relax the assumption that we know the variance of growth between companies, $\sigma^2$, and estimate that variance. Instead of the t prior from earlier, we will use the conditionally conjugate priors, normal for $\mu$ and inverse-gamma for $\sigma^2$

The first step will be to write functions to simulate from the full conditional distributions we derived in the previous segment. The full conditional for $\mu$, given $\sigma^2$ and data is

$\text{N} \left( \mu \mid \frac{n\bar{y}/\sigma^2 + \mu_0/\sigma_0^2}{n/\sigma^2 + 1/\sigma_0^2}, \, \frac{1}{n/\sigma^2 + 1/\sigma_0^2} \right)$

```{r,echo=FALSE}
update_mu = function(n, ybar, sig2, mu_0, sig2_0) {
  sig2_1 = 1.0 / (n / sig2 + 1.0 / sig2_0)
  mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)
  rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))
}
```

The full conditional for $\sigma^2$ given $\mu$ and data is:

$\text{IG}\left( \sigma^2 \mid \nu_0 + \frac{n}{2}, \, \beta_0 + \frac{\sum_{i=1}^n (y_i - \mu)^2}{2} \right)$

```{r,echo=FALSE}
update_sig2 = function(n, y, mu, nu_0, beta_0) {
  nu_1 = nu_0 + n / 2.0
  sumsq = sum( (y - mu)^2 ) # vectorized
  beta_1 = beta_0 + sumsq / 2.0
  out_gamma = rgamma(n=1, shape=nu_1, rate=beta_1) # rate for gamma is shape for inv-gamma
  1.0 / out_gamma # reciprocal of a gamma random variable is distributed inv-gamma
}
```

With functions for drawing from the full conditionals, we are ready to write a function to perform Gibbs sampling.

```{r,echo=FALSE}
gibbs = function(y, n_iter, init, prior) {
  ybar = mean(y)
  n = length(y)
  
  ## initialize
  mu_out = numeric(n_iter)
  sig2_out = numeric(n_iter)
  
  mu_now = init$mu
  
  ## Gibbs sampler
  for (i in 1:n_iter) {
    sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)
    mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)
    
    sig2_out[i] = sig2_now
    mu_out[i] = mu_now
  }
  
  cbind(mu=mu_out, sig2=sig2_out)
}
```

Now we are ready to set up the problem in $\text{R}$.

```{r ,echo=FALSE }
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)

## prior
prior = list()
prior$mu_0 = 0.0
prior$sig2_0 = 1.0
prior$n_0 = 2.0 # prior effective sample size for sig2
prior$s2_0 = 1.0 # prior point estimate for sig2
prior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma
prior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma

hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dnorm(x=x, mean=prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
```

```{r,comment="",echo=FALSE }
set.seed(53)

init = list()
init$mu = 0.0

post = gibbs(y=y, n_iter=1e3, init=init, prior=prior)
head(post)

library("coda")
plot(as.mcmc(post))


summary(as.mcmc(post))
```

As with the Metropolis-Hastings example, these chains appear to have converged. In the next lesson, we will discuss convergence in more detail.


\newpage


# Popular one level models 

## ANOVA

As an example of a one-way ANOVA, we’ll look at the Plant Growth data in R.

```{r}
data("PlantGrowth")

head(PlantGrowth)

```


Because the explanatory variable group is a factor and not continuous, we choose to visualize the data with box plots rather than scatter plots.

```{r}
boxplot(weight ~ group, data=PlantGrowth)
```


The box plots summarize the distribution of the data for each of the three groups. It appears that treatment 2 has the highest mean yield. It might be questionable whether each group has the same variance, but we’ll assume that is the case.

**Modeling**
Again, we can start with the reference analysis (with a noninformative prior) with a linear model in R.

```{r,comment="",echo=FALSE }
lmod = lm(weight ~ group, data=PlantGrowth)
summary(lmod)

par(mfrow=c(2,2))
plot(lmod) # for graphical residual analysis
```

```{r,comment="",echo=FALSE }
anova(lmod)
```

The default model structure in R is the linear model with dummy indicator variables. Hence, the “intercept” in this model is the mean yield for the control group. The two other parameters are the estimated effects of treatments 1 and 2. To recover the mean yield in treatment group 1, you would add the intercept term and the treatment 1 effect. To see how R sets the model up, use the model.matrix(lmod) function to extract the X matrix.

The anova() function in R compares variability of observations between the treatment groups to variability within the treatment groups to test whether all means are equal or whether at least one is different. The small p-value here suggests that the means are not all equal.

Let’s fit the cell means model in JAGS.


```{r,comment="",echo=FALSE }

library("rjags")

mod_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dnorm(mu[grp[i]], prec)
    }
    
    for (j in 1:3) {
        mu[j] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(5/2.0, 5*1.0/2.0)
    sig = sqrt( 1.0 / prec )
} "

set.seed(82)

data_jags = list(y=PlantGrowth$weight, 
              grp=as.numeric(PlantGrowth$group))

params = c("mu", "sig")

inits = function() {
    inits = list("mu"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}

mod = jags.model(textConnection(mod_string), data=data_jags, inits=inits, n.chains=3)
update(mod, 1e3)

mod_sim = coda.samples(model=mod,
                        variable.names=params,
                        n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim)) # combined chains
```

**Model checking**
As usual, we check for convergence of our MCMC.

```{r,comment="",echo=FALSE,fig.height=9 }
plot(mod_sim)

gelman.diag(mod_sim)
autocorr.diag(mod_sim)
effectiveSize(mod_sim)


(pm_params = colMeans(mod_csim))
```

```{r,comment="",echo=FALSE,fig.height=9 }
yhat = pm_params[1:3][data_jags$grp]
resid = data_jags$y - yhat
plot(resid)
```

```{r,comment="",echo=FALSE,fig.height=9 }

plot(yhat, resid)
```

Again, it might be appropriate to have a separate variance for each group. We will have you do that as an exercise.

**Results**

Let’s look at the posterior summary of the parameters.

```{r,comment="",echo=FALSE,fig.height=9 }
summary(mod_sim)
```

```{r,comment="",echo=FALSE,fig.height=9 }
HPDinterval(mod_csim)
```

The HPDinterval() function in the coda package calculates intervals of highest posterior density for each parameter.

We are interested to know if one of the treatments increases mean yield. It is clear that treatment 1 does not. What about treatment 2?

```{r,comment="",echo=FALSE,fig.height=9 }
mean(mod_csim[,3] > mod_csim[,1])
```

There is a high posterior probability that the mean yield for treatment 2 is greater than the mean yield for the control group.

It may be the case that treatment 2 would be costly to put into production. Suppose that to be worthwhile, this treatment must increase mean yield by 10%. What is the posterior probability that the increase is at least that?

```{r,comment="",echo=FALSE,fig.height=9 }
mean(mod_csim[,3] > 1.1*mod_csim[,1])
```

We have about 50/50 odds that adopting treatment 2 would increase mean yield by at least 10%.


\newpage

## MANOVA

Let’s explore an example with two factors. We’ll use the Warpbreaks data set in R. Check the documentation for a description of the data by typing ?warpbreaks.

```{r,echo=FALSE, comment=""}
data("warpbreaks")

head(warpbreaks)
```

Again, we visualize the data with box plots.

```{r,echo=FALSE, comment=""}
table(warpbreaks$wool, warpbreaks$tension)

boxplot(breaks ~ wool + tension, data=warpbreaks)

```

```{r,echo=FALSE, comment=""}
boxplot(log(breaks) ~ wool + tension, data=warpbreaks)
```

The different groups have more similar variance if we use the logarithm of breaks. From this visualization, it looks like both factors may play a role in the number of breaks. It appears that there is a general decrease in breaks as we move from low to medium to high tension. Let’s start with a one-way model using tension only.

### One-way model

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE,fig.height=9, echo=FALSE}
library("rjags")
mod1_string = " model {
    for( i in 1:length(y)) {
        y[i] ~ dnorm(mu[tensGrp[i]], prec)
    }
    
    for (j in 1:3) {
        mu[j] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(5/2.0, 5*2.0/2.0)
    sig = sqrt(1.0 / prec)
} "

set.seed(83)
str(warpbreaks)

data1_jags = list(y=log(warpbreaks$breaks), tensGrp=as.numeric(warpbreaks$tension))

params1 = c("mu", "sig")

mod1 = jags.model(textConnection(mod1_string), data=data1_jags, n.chains=3)
update(mod1, 1e3)

mod1_sim = coda.samples(model=mod1,
                        variable.names=params1,
                        n.iter=5e3)

## convergence diagnostics
plot(mod1_sim)

gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
effectiveSize(mod1_sim)
```

Here are the results.

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
summary(mod1_sim)
```

The 95% posterior interval for the mean of group 2 (medium tension) overlaps with both the low and high groups, but the low and high groups do not have overlapping intervals. That is a pretty strong indication that the means for low and high tension are different. Let’s collect the DIC for this model and move on to the two-way model.

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
dic1 = dic.samples(mod1, n.iter=1e3)
```

### Two-way additive model
With two factors, one with two levels and the other with three, we have six treatment groups, which is the same situation we discussed when introducing multiple factor ANOVA. We will first fit the additive model which treats the two factors separately with no interaction. To get the XX matrix (or design matrix) for this model, we can extract it from a linear model in R.

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
X = model.matrix(lm(breaks ~ wool + tension, data=warpbreaks))
head(X)
```


```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
tail(X)
```

By default, R has chosen the mean for wool A and low tension to be the intercept. Then, there is an effect for wool B, and effects for medium tension and high tension, each associated with dummy indicator variables.

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
mod2_string = " model {
    for( i in 1:length(y)) {
        y[i] ~ dnorm(mu[i], prec)
        mu[i] = int + alpha*isWoolB[i] + beta[1]*isTensionM[i] + beta[2]*isTensionH[i]
    }
    
    int ~ dnorm(0.0, 1.0/1.0e6)
    alpha ~ dnorm(0.0, 1.0/1.0e6)
    for (j in 1:2) {
        beta[j] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(3/2.0, 3*1.0/2.0)
    sig = sqrt(1.0 / prec)
} "

data2_jags = list(y=log(warpbreaks$breaks), isWoolB=X[,"woolB"], isTensionM=X[,"tensionM"], isTensionH=X[,"tensionH"])

params2 = c("int", "alpha", "beta", "sig")

mod2 = jags.model(textConnection(mod2_string), data=data2_jags, n.chains=3)
update(mod2, 1e3)

mod2_sim = coda.samples(model=mod2,
                        variable.names=params2,
                        n.iter=5e3)

## convergene diagnostics
plot(mod2_sim)

gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
effectiveSize(mod1_sim)
```

Let’s summarize the results, collect the DIC for this model, and compare it to the first one-way model.

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
summary(mod2_sim)
(dic2 = dic.samples(mod2, n.iter=1e3))
dic1
```

This suggests we haven’t gained much from adding the wool factor to the model. Before we discard wool, however, we should consider whether there is an interaction. Let’s look again at the box plot with all six treatment groups.

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
boxplot(log(breaks) ~ wool + tension, data=warpbreaks)
```

Our two-way model has a single effect for wool B and the estimate is negative. If this is true, then we would expect wool B to be associated with fewer breaks than its wool A counterpart on average. This is true for low and high tension, but it appears that breaks are higher for wool B when there is medium tension. That is, the effect for wool B is not consistent across tension levels, so it may appropriate to add an interaction term. In  R, this would look like:

```{r, echo=FALSE,comment="", warning=FALSE,message=FALSE, echo=FALSE}
lmod2 = lm(log(breaks) ~ .^2, data=warpbreaks)
summary(lmod2)
```

Adding the interaction, we get an effect for being in wool B and medium tension, as well as for being in wool B and high tension. There are now six parameters for the mean, one for each treatment group, so this model is equivalent to the full cell means model. Let’s use that.


### Two-way cell means model

In this new model, $\mu$ will be a matrix with six entries, each corresponding to a treatment group.


```{r, echo=FALSE,fig.height=9,comment="", warning=FALSE,message=FALSE, echo=FALSE}

mod3_string = " model {
    for( i in 1:length(y)) {
        y[i] ~ dnorm(mu[woolGrp[i], tensGrp[i]], prec)
    }
    
    for (j in 1:max(woolGrp)) {
        for (k in 1:max(tensGrp)) {
            mu[j,k] ~ dnorm(0.0, 1.0/1.0e6)
        }
    }
    
    prec ~ dgamma(3/2.0, 3*1.0/2.0)
    sig = sqrt(1.0 / prec)
} "

str(warpbreaks)

data3_jags = list(y=log(warpbreaks$breaks), woolGrp=as.numeric(warpbreaks$wool), tensGrp=as.numeric(warpbreaks$tension))

params3 = c("mu", "sig")

mod3 = jags.model(textConnection(mod3_string), data=data3_jags, n.chains=3)
update(mod3, 1e3)

mod3_sim = coda.samples(model=mod3,
                        variable.names=params3,
                        n.iter=5e3)
mod3_csim = as.mcmc(do.call(rbind, mod3_sim))

plot(mod3_sim, ask=TRUE)

## convergence diagnostics
gelman.diag(mod3_sim)
autocorr.diag(mod3_sim)
effectiveSize(mod3_sim)
raftery.diag(mod3_sim)
```

Let’s compute the DIC and compare with our previous models.

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE}
(dic3 = dic.samples(mod3, n.iter=1e3))
dic2
dic1
```

This suggests that the full model with interaction between wool and tension is the best for explaining/predicting warp breaks.

### Results

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE}
summary(mod3_sim)
HPDinterval(mod3_csim)
par(mfrow=c(3,2)) # arrange frame for plots
densplot(mod3_csim[,1:6], xlim=c(2.0, 4.5))
```

It might be tempting to look at comparisons between each combination of treatments, but we warn that this could yield spurious results. When we discussed the statistical modeling cycle, we said it is best not to search your results for interesting hypotheses, because if there are many hypotheses, some will appear to show “effects” or “associations” simply due to chance. Results are most reliable when we determine a relatively small number of hypotheses we are interested in beforehand, collect the data, and statistically evaluate the evidence for them.

One question we might be interested in with these data is finding the treatment combination that produces the fewest breaks. To calculate this, we can go through our posterior samples and for each sample, find out which group has the smallest mean. These counts help us determine the posterior probability that each of the treatment groups has the smallest mean.

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE}
prop.table( table( apply(mod3_csim[,1:6], 1, which.min) ) )
```

The evidence supports wool B with high tension as the treatment that produces the fewest breaks.

\newpage

## Linear Regression

As an example of linear regression, we’ll look at the Leinhardt data from the car package in R.

```{r,echo=FALSE, comment=""}
library("car")
Lein=car::Leinhardt
head(Lein)
```

So the Leinhardt dataset has 105 observations and 4 variables: `r names(Lein)`.

```{r,echo=FALSE, comment=""}
str(Lein)
```

So the correlation between the variables is shown by the following paired scatterplot.

```{r,echo=FALSE, comment=""}
pairs(Lein)
```

We’ll start with a simple linear regression model that relates infant mortality to per capita income, but first let's take a look if there is a linear relation between the two variables. This can be easily tested by a scatterplot.

```{r,echo=FALSE, comment=""}
plot(infant ~ income, data=Lein)
```

```{r,echo=FALSE, comment=""}
hist(Leinhardt$infant, main = "Histogram of infant mortality")
hist(Leinhardt$income, main = "Histogram of income")
```

As we can easily observe our setting is not suitable for a linear regression model, because the variables are heavily skewed, plus the scatterplot shows no linearity. This can be corrected in some cases when we use the log transformation : 

```{r,echo=FALSE, comment=""}
Leinhardt$loginfant = log(Leinhardt$infant)
Leinhardt$logincome = log(Leinhardt$income)

plot(loginfant ~ logincome, data=Leinhardt)
```

 A linear model appears much more appropriate on this (log) scale.
 The first model we may apply is the frequentist (non informative Bayesian) linear model. 
 
```{r,echo=FALSE, comment=""}
lmod = lm(loginfant ~ logincome, data=Leinhardt)
summary(lmod)

```

Let's check also the model fit. 

```{r,echo=FALSE, comment="",warning=FALSE, fig.height=8}
par(mfrow=c(2,2))
plot(lmod)
dev.off()
```

Now let's fit the same model using JAGS. We can also ommit the some countries with missing values for easier calculations.

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE}
dat = na.omit(Leinhardt)



mod1_string = " model {
    for (i in 1:n) {
        y[i] ~ dnorm(mu[i], prec) ### y comes from a normal distribution with mu and prec
        mu[i] = b[1] + b[2]*log_income[i] ## The mu is calculated as a linear prediction
    }
    
    for (i in 1:2) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)  ## Both coefficients have uninformative priors
    }
    
    prec ~ dgamma(5/2.0, 5*10.0/2.0) ## precision is modeled from a gamma distribution
    sig2 = 1.0 / prec                ## So variance is from inverse gamma
    sig = sqrt(sig2)                 ## and the standard deviation too 
} "

set.seed(72)
data1_jags = list(y=dat$loginfant, n=nrow(dat), 
              log_income=dat$logincome)

params1 = c("b", "sig")

inits1 = function() {
    inits = list("b"=rnorm(2,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}

mod1 = jags.model(textConnection(mod1_string), data=data1_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in

mod1_sim = coda.samples(model=mod1,
                        variable.names=params1,
                        n.iter=5000)

mod1_csim = do.call(rbind, mod1_sim) # combine multiple chains
```

Before we check the inferences from the model, we should perform convergence diagnostics for our Markov chains.

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE, comment=""}
plot(mod1_sim)
gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
autocorr.plot(mod1_sim)
effectiveSize(mod1_sim)

summary(mod1_sim)
```


### Residual checking

Checking residuals (the difference between the response and the model’s prediction for that value) is important with linear models since residuals can reveal violations of the assumptions we made to specify the model. In particular, we are looking for any sign that the model is not linear, normally distributed, or that the observations are not independent (conditional on covariates).

First, let’s look at what would have happened if we fit the reference linear model to the un-transformed variables.

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE, comment="", fig.height=7}
lmod0 = lm(infant ~ income, data=Leinhardt)
plot(resid(lmod0)) # to check independence (looks okay)


plot(predict(lmod0), resid(lmod0)) # to check for linearity, constant variance (looks bad)

qqnorm(resid(lmod0)) # to check Normality assumption (we want this to be a straight line)
```



Now let’s return to our model fit to the log-transformed variables. In a Bayesian model, we have distributions for residuals, but we’ll simplify and look only at the residuals evaluated at the posterior mean of the parameters.

```{r, echo=FALSE, warning=FALSE,message=FALSE, echo=FALSE, comment=""}
X = cbind(rep(1.0, data1_jags$n), data1_jags$log_income)
head(X)
(pm_params1 = colMeans(mod1_csim)) # posterior mean


yhat1 = drop(X %*% pm_params1[1:2])
resid1 = data1_jags$y - yhat1
plot(resid1) # against data index


plot(yhat1, resid1) # against predicted values

qqnorm(resid1) # checking normality of residuals

plot(predict(lmod), resid(lmod)) # to compare with reference linear model

rownames(dat)[order(resid1, decreasing=TRUE)[1:5]] # which countries have the largest positive residuals?
```

The residuals look pretty good here (no patterns, shapes) except for two strong outliers, Saudi Arabia and Libya. When outliers appear, it is a good idea to double check that they are not just errors in data entry. If the values are correct, you may reconsider whether these data points really are representative of the data you are trying to model. If you conclude that they are not (for example, they were recorded on different years), you may be able to justify dropping these data points from the data set.

If you conclude that the outliers are part of data and should not be removed, we have several modeling options to accommodate them. We will address these in the next segment.

### Adding covariates

The first approach is to look for additional covariates that may be able to explain the outliers. For example, there could be a number of variables that provide information about infant mortality above and beyond what income provides.

Looking back at our data, there are two variables we haven’t used yet: region and oil. The oil variable indicates oil-exporting countries. Both Saudi Arabia and Libya are oil-exporting countries, so perhaps this might explain part of the anomaly.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
library("rjags")
set.seed(75)
mod2_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dnorm(mu[i], prec)
        mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]
    }
    
    for (i in 1:3) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(5/2.0, 5*10.0/2.0)
    sig = sqrt( 1.0 / prec )
} "


set.seed(75)
data2_jags = list(y=dat$loginfant, log_income=dat$logincome,
                  is_oil=as.numeric(dat$oil=="yes"))
data2_jags$is_oil

params2 = c("b", "sig")

inits2 = function() {
    inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}

mod2 = jags.model(textConnection(mod2_string), data=data2_jags, inits=inits2, n.chains=3)
update(mod2, 1e3) # burn-in

mod2_sim = coda.samples(model=mod2,
                        variable.names=params2,
                        n.iter=5e3)

mod2_csim = as.mcmc(do.call(rbind, mod2_sim)) # combine multiple chains
```

As usual, check the convergence diagnostics.

```{r,echo=FALSE,fig.height=9, warning=FALSE, message=FALSE, comment=""}
plot(mod2_sim)
```

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
gelman.diag(mod2_sim)
autocorr.diag(mod2_sim)

autocorr.plot(mod2_sim)
effectiveSize(mod2_sim)
summary(mod2_sim)
```

It looks like there is a positive relationship between oil-production and log-infant mortality. Because these data are merely observational, we cannot say that oil-production causes an increase in infant mortality (indeed that most certainly isn’t the case), but we can say that they are positively correlated.

Now let’s check the residuals.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
X2 = cbind(rep(1.0, data1_jags$n), data2_jags$log_income, data2_jags$is_oil)
head(X2)

```

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
(pm_params2 = colMeans(mod2_csim)) # posterior mean

yhat2 = drop(X2 %*% pm_params2[1:3])
resid2 = data2_jags$y - yhat2
plot(resid2) # against data index
```


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment="", fig.height=9}
par(mfrow=c(2,1))
plot(yhat1, resid1, main = "Residuals from the first model") # residuals from the first model
plot(yhat2, resid2, main= "Residuals from the second model") # against predicted values

sd(resid2) # standard deviation of residuals
```

These look much better, although the residuals for Saudi Arabia and Libya are still more than three standard deviations away from the mean of the residuals. We might consider adding the other covariate region, but instead let’s look at another option when we are faced with strong outliers.

**t likelihood**

Let’s consider changing the likelihood. The normal likelihood has thin tails (almost all of the probability is concentrated within the first few standard deviations from the mean). This does not accommodate outliers well. Consequently, models with the normal likelihood might be overly-influenced by outliers. Recall that the tt distribution is similar to the normal distribution, but it has thicker tails which can accommodate outliers.

The t linear model might look something like this. Notice that the tt distribution has three parameters, including a positive “degrees of freedom” parameter. The smaller the degrees of freedom, the heavier the tails of the distribution. We might fix the degrees of freedom to some number, or we can assign it a prior distribution.


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment="", fig.height=9}
mod3_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dt( mu[i], tau, df )
        mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil
    }
    
    for (i in 1:3) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    df = nu + 2.0 # we want degrees of freedom > 2 to guarantee existence of mean and variance
    nu ~ dexp(1.0)
    
    tau ~ dgamma(5/2.0, 5*10.0/2.0) # tau is close to, but not equal to the precision
    sig = sqrt( 1.0 / tau * df / (df - 2.0) ) # standard deviation of errors
} "


```



### Compare models using Deviance Information Criterion

We have now proposed three different models. How do we compare their performance on our data? In the previous course, we discussed estimating parameters in models using the maximum likelihood method. Similarly, we can choose between competing models using the same idea.

We will use a quantity known as the deviance information criterion (DIC). It essentially calculates the posterior mean of the log-likelihood and adds a penalty for model complexity.

Let’s calculate the DIC for our first two models:

the simple linear regression on log-income,
```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
dic.samples(mod1, n.iter=1e3)
dic.samples(mod2, n.iter=1e3)
```

The first number is the Monte Carlo estimated posterior mean deviance, which equals -2 times the log-likelihood (plus a constant that will be irrelevant for comparing models). Because of that -2 factor, a smaller deviance means a higher likelihood.

Next, we are given a penalty for the complexity of our model. This penalty is necessary because we can always increase the likelihood of the model by making it more complex to fit the data exactly. We don’t want to do this because over-fit models generalize poorly. This penalty is roughly equal to the effective number of parameters in your model. You can see this here. With the first model, we had a variance parameter and two betas, for a total of three parameters. In the second model, we added one more beta for the oil effect.

We add these two quantities to get the DIC (the last number). The better-fitting model has a lower DIC value. In this case, the gains we receive in deviance by adding the is_oil covariate outweigh the penalty for adding an extra parameter. The final DIC for the second model is lower than for the first, so we would prefer using the second model.

We encourage you to explore different model specifications and compare their fit to the data using DIC. Wikipedia provides a good introduction to DIC and we can find more details about the JAGS implementation through the rjags package documentation by entering ?dic.samples in the  R console.

\newpage

# Linear Regression 2nd example

New York City Crime Data

As an example of a Bayesian linear regression model, we look at New York City crime data from 1966 to 1967. The outcome variable (THEFT) is the increase or decrease in the seasonally adjusted rate of grand larcenies in 23 Manhattan police precincts from a 27-week pre-intervention period compared to a 58-week intervention period. The predictor variables are the percent increase or decrease in the number of police officers in a precinct (MAN), and whether the precinct was located uptown, midtown or downtown.

We specify the model as:


$$THEFT_i \sim N(\mu, \sigma^) \\
\mu = \beta_0 + \beta_1 * MAN + district \\
\frac{1}{\sigma^2} \sim \Gamma(0.001, 0.001) \\
\beta_0 \sim N(0, 100000) \\
\beta_1 \sim N(0, 100000)$$

We will need a prior for the effect of geographic area (district), but will discuss that in a moment. First, we consider how best to code an indicator variable in BUGS. There are two possible approaches. We can create a “design matrix” of dummy variables. In this approach we create two variables named DIST1 and DIST2, for which downtown precincts are coded 0,0 , midtown precincts are coded 1,0 and uptown precincts are coded 0,1. The BUGS code for this model would then be:





\newpage




# Logistic regression


For an example of logistic regression, we’ll use the urine data set from the boot package in R. The response variable is r, which takes on values of 0 or 1. We will remove some rows from the data set which contain missing values.


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
library("boot")
data("urine")
dat = na.omit(urine)
head(urine)

```

Let’s look at pairwise scatter plots of the seven variables.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
pairs(dat)
```

One thing that stands out is that several of these variables are strongly correlated with one another. For example gravity and osmo appear to have a very close linear relationship. Collinearity between xx variables in linear regression models can cause trouble for statistical inference. Two correlated variables will compete for the ability to predict the response variable, leading to unstable estimates. This is not a problem for prediction of the response, if prediction is the end goal of the model. But if our objective is to discover how the variables relate to the response, we should avoid collinearity.

We can more formally estimate the correlation among these variables using the corrplot package.


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
library("corrplot")
Cor = cor(dat)
corrplot(Cor, type="upper", method="ellipse", tl.pos="d")
corrplot(Cor, type="lower", method="number", col="black", 
         add=TRUE, diag=FALSE, tl.pos="n", cl.pos="n")
```

### Variable selection
One primary goal of this analysis is to find out which variables are related to the presence of calcium oxalate crystals. This objective is often called “variable selection.” We have already seen one way to do this: fit several models that include different sets of variables and see which one has the best DIC. Another way to do this is to use a linear model where the priors for the $\beta$ coefficients favor values near 0 (indicating a weak relationship). This way, the burden of establishing association lies with the data. If there is not a strong signal, we assume it doesn’t exist.

Rather than tailoring a prior for each individual $\beta$ based on the scale its covariate takes values on, it is customary to subtract the mean and divide by the standard deviation for each variable.


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
X = scale(dat[,-1], center=TRUE, scale=TRUE)
head(X[,"gravity"])

```

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
colMeans(X)
apply(X, 2, sd)
```

### Model
Our prior for the $\beta$ (which we’ll call bb in the model) coefficients will be the double exponential (or Laplace) distribution, which as the name implies, is the exponential distribution with tails extending in the positive direction as well as the negative direction, with a sharp peak at 0. We can read more about it in the JAGS manual. The distribution looks like:

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
ddexp = function(x, mu, tau) {
  0.5*tau*exp(-tau*abs(x-mu)) 
}
curve(ddexp(x, mu=0.0, tau=1.0), from=-5.0, to=5.0, ylab="density", main="Double exponential\ndistribution") # double exponential distribution
curve(dnorm(x, mean=0.0, sd=1.0), from=-5.0, to=5.0, lty=2, add=TRUE) # normal distribution
legend("topright", legend=c("double exponential", "normal"), lty=c(1,2), bty="n")
```

```{r,echo=FALSE,fig.height=9, warning=FALSE, message=FALSE, comment=""}
library("rjags")
mod1_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])
        logit(p[i]) = int + b[1]*gravity[i] + b[2]*ph[i] + b[3]*osmo[i] + b[4]*cond[i] + b[5]*urea[i] + b[6]*calc[i]
    }
    int ~ dnorm(0.0, 1.0/25.0)
    for (j in 1:6) {
        b[j] ~ ddexp(0.0, sqrt(2.0)) # has variance 1.0
    }
} "

set.seed(92)
head(X)

data_jags = list(y=dat$r, gravity=X[,"gravity"], ph=X[,"ph"], osmo=X[,"osmo"], cond=X[,"cond"], urea=X[,"urea"], calc=X[,"calc"])

params = c("int", "b")

mod1 = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)
update(mod1, 1e3)

mod1_sim = coda.samples(model=mod1,
                        variable.names=params,
                        n.iter=5e3)
mod1_csim = as.mcmc(do.call(rbind, mod1_sim))

## convergence diagnostics
plot(mod1_sim, ask=TRUE)

gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
autocorr.plot(mod1_sim)
effectiveSize(mod1_sim)

## calculate DIC
dic1 = dic.samples(mod1, n.iter=1e3)
```

Let’s look at the results.



```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
summary(mod1_sim)
par(mfrow=c(3,2))
densplot(mod1_csim[,1:6], xlim=c(-3.0, 3.0))
```

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
colnames(X) # variable names
```

It is clear that the coefficients for variables gravity, cond (conductivity), and calc (calcium concentration) are not 0. The posterior distribution for the coefficient of osmo (osmolarity) looks like the prior, and is almost centered on 0 still, so we’ll conclude that osmo is not a strong predictor of calcium oxalate crystals. The same goes for ph.

urea (urea concentration) appears to be a borderline case. However, if we refer back to our correlations among the variables, we see that urea is highly correlated with gravity, so we opt to remove it.

Our second model looks like this:

```{r,echo=FALSE, warning=FALSE,fig.height=9, message=FALSE, comment=""} 
mod2_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])
        logit(p[i]) = int + b[1]*gravity[i] + b[2]*cond[i] + b[3]*calc[i]
    }
    int ~ dnorm(0.0, 1.0/25.0)
    for (j in 1:3) {
        b[j] ~ dnorm(0.0, 1.0/25.0) # noninformative for logistic regression
    }
} "

mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)
update(mod2, 1e3)

mod2_sim = coda.samples(model=mod2,
                        variable.names=params,
                        n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))

plot(mod2_sim, ask=TRUE)

gelman.diag(mod2_sim)
autocorr.diag(mod2_sim)
autocorr.plot(mod2_sim)
effectiveSize(mod2_sim)

dic2 = dic.samples(mod2, n.iter=1e3)
```

### Results


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
dic1
dic2
summary(mod2_sim)
```

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
HPDinterval(mod2_csim)
```


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
par(mfrow=c(3,1))
densplot(mod2_csim[,1:3], xlim=c(-3.0, 3.0))
colnames(X)[c(1,4,6)] # variable names
```

The DIC is actually better for the first model. Note that we did change the prior between models, and generally we should not use the DIC to choose between priors. Hence comparing DIC between these two models may not be a fair comparison. Nevertheless, they both yield essentially the same conclusions. Higher values of gravity and calc (calcium concentration) are associated with higher probabilities of calcium oxalate crystals, while higher values of cond (conductivity) are associated with lower probabilities of calcium oxalate crystals. There are more modeling options in this scenario, perhaps including transformations of variables, different priors, and interactions between the predictors, but we’ll leave it to you to see if you can improve the model.

### Prediction from a logisic regression model

How do we turn model parameter estimates into model predictions? The key is the form of the model. Remember that the likelihood is Bernoulli, which is 1 with probability pp. We modeled the logit of pp as a linear model, which we showed in the first segment of this lesson leads to an exponential form for E(y)=p(y)=p.

Take the output from our model in the last segment. We will use the posterior means as point estimates of the parameters.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
(pm_coef = colMeans(mod2_csim))
```

The posterior mean of the intercept was about -0.15. Since we centered and scaled all of the covariates, values of 0 for each xx correspond to the average values. Therefore, if we use our last model, then our point estimate for the probability of calcium oxalate crystals when gravity,  cond, and calc are at their average values is 1/(1+e^{-(-0.15)})== 0.4625702.

Now suppose we want to make a prediction for a new specimen whose value of gravity is average, whose value of cond is one standard deviation below the mean, and whose value of calc is one standard deviation above the mean. Our point estimate for the probability of calcium oxalate crystals is 1/(1+e^{-(-0.15 + 1.4*0.0 - 1.3*(-1.0) + 1.9*(1.0))})=  0.9547825.

If we want to make predictions in terms of the original xx variable values, we have two options:

1. For each x variable, subtract the mean and divide by the standard deviation for that variable in the original data set used to fit the model.

2. Re-fit the model without centering and scaling the covariates.

#### Predictive checks

We can use the same ideas to make predictions for each of the original data points. This is similar to what we did to calculate residuals with earlier models.

First we take the X matrix and matrix multiply it with the posterior means of the coefficients. Then we need to pass these linear values through the inverse of the link function as we did above.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""} 
pm_Xb = pm_coef["int"] + X[,c(1,4,6)] %*% pm_coef[1:3]
phat = 1.0 / (1.0 + exp(-pm_Xb))
head(phat)
```

These phat values are the model’s predicted probability of calcium oxalate crystals for each data point. We can get a rough idea of how successful the model is by plotting these predicted values against the actual outcome.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
plot(phat, jitter(dat$r))
```

Suppose we choose a cutoff for these predicted probabilities. If the model tells us the probability is higher than 0.5, we will classify the observation as a 1 and if it is less than 0.5, we will classify it as a 0. That way the model classifies each data point. Now we can tabulate these classifications against the truth to see how well the model predicts the original data.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
(tab0.5 = table(phat > 0.5, data_jags$y))
```

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
sum(diag(tab0.5)) / sum(tab0.5)
```

The correct classification rate is about 76%, not too bad, but not great.

Now suppose that it is considered really bad to predict no calcium oxalate crystal when there in fact is one. We might then choose to lower our threshold for classifying data points as 1s. Say we change it to 0.3. That is, if the model says the probability is greater than 0.3, we will classify it as having a calcium oxalate crystal.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
(tab0.3 = table(phat > 0.3, data_jags$y))
sum(diag(tab0.3)) / sum(tab0.3)
```

It looks like we gave up a little classification accuracy, but we did indeed increase our chances of detecting a true positive.

We could repeat this exercise for many thresholds between 0 and 1, and each time calculate our error rates. This is equivalent to calculating what is called the ROC (receiver-operating characteristic) curve, which is often used to evaluate classification techniques.

These classification tables we have calculated were all in-sample. They were predicting for the same data used to fit the model. We could get a less biased assessment of how well our model performs if we calculated these tables for data that were not used to fit the model. For example, before fitting the model, you could withhold a set of randomly selected “test” data points, and use the model fit to the rest of the “training” data to make predictions on your “test” set.



\newpage

## Poisson regression
**Data**

For an example of Poisson regression, we’ll use the badhealth data set from the COUNT package in R.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
library("COUNT")
data("badhealth")
head(badhealth)
any(is.na(badhealth))
```

As usual, let’s visualize these data.

```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
hist(badhealth$numvisit, breaks=20)
```

```{r}
plot(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==0, xlab="age", ylab="log(visits)")
points(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==1, col="red")
```

#### Model
It appears that both age and bad health are related to the number of doctor visits. We should include model terms for both variables. If we believe the age/visits relationship is different between healthy and non-healthy populations, we should also include an interaction term. We will fit the full model here and leave it to you to compare it with the simpler additive model.

```{r}
library("rjags")
mod_string = " model {
    for (i in 1:length(numvisit)) {
        numvisit[i] ~ dpois(lam[i])
        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]
    }
    
    int ~ dnorm(0.0, 1.0/1e6)
    b_badh ~ dnorm(0.0, 1.0/1e4)
    b_age ~ dnorm(0.0, 1.0/1e4)
    b_intx ~ dnorm(0.0, 1.0/1e4)
} "

set.seed(102)

data_jags = as.list(badhealth)

params = c("int", "b_badh", "b_age", "b_intx")

mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)

mod_sim = coda.samples(model=mod,
                        variable.names=params,
                        n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))

## convergence diagnostics
plot(mod_sim)

gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)

## compute DIC
dic = dic.samples(mod, n.iter=1e3)
```

#### Model checking
To get a general idea of the model’s performance, we can look at predicted values and residuals as usual. Don’t forget that we must apply the inverse of the link function to get predictions for $\lambda$.



```{r}
X = as.matrix(badhealth[,-1])
X = cbind(X, with(badhealth, badh*age))
head(X)
```

```{r}
(pmed_coef = apply(mod_csim, 2, median))
```

```{r}
llam_hat = pmed_coef["int"] + X %*% pmed_coef[c("b_badh", "b_age", "b_intx")]
lam_hat = exp(llam_hat)

hist(lam_hat)
```


```{r}
resid = badhealth$numvisit - lam_hat
plot(resid) # the data were ordered
```


```{r}
plot(lam_hat, badhealth$numvisit)
abline(0.0, 1.0)
```

It is not surprising that the variability increases for values predicted at higher values since the mean is also the variance in the Poisson distribution. However, observations predicted to have about two visits should have variance about two, and observations predicted to have about six visits should have variance about six.

```{r}
plot(lam_hat[which(badhealth$badh==0)], resid[which(badhealth$badh==0)], xlim=c(0, 8), ylab="residuals", xlab=expression(hat(lambda)), ylim=range(resid))
points(lam_hat[which(badhealth$badh==1)], resid[which(badhealth$badh==1)], col="red")
```


```{r}
var(resid[which(badhealth$badh==0)])
```

```{r}
var(resid[which(badhealth$badh==1)])
```

Clearly this is not the case with these data. This indicates that either the model fits poorly (meaning the covariates don’t explain enough of the variability in the data), or the data are “overdispersed” for the Poisson likelihood we have chosen. This is a common issue with count data. If the data are more variable than the Poisson likelihood would suggest, a good alternative is the negative binomial distribution, which we will not pursue here.

#### Results

Assuming the model fit is adequate, we can interpret the results.


```{r}
summary(mod_sim)
```

The intercept is not necessarily interpretable here because it corresponds to a healthy 0-year-old, whereas the youngest person in the data set is 20 years old.

For healthy individuals, it appears that age has a positive association with number of doctor visits. Clearly, bad health is associated with an increase in expected number of visits. The interaction coefficient is interpreted as an adjustment to the age coefficient for people in bad health. Hence, for people with bad health, age is essentially unassociated with number of visits.

#### Predictive distributions
Let’s say we have two people aged 35, one in good health and the other in poor health. What is the posterior probability that the individual with poor health will have more doctor visits? This goes beyond the posterior probabilities we have calculated comparing expected responses in previous lessons. Here we will create Monte Carlo samples for the responses themselves. This is done by taking the Monte Carlo samples of the model parameters, and for each of those, drawing a sample from the likelihood. Let’s walk through this.

First, we need the xx values for each individual. We’ll say the healthy one is Person 1 and the unhealthy one is Person 2. Their xx values are

```{r}
x1 = c(0, 35, 0) # good health
x2 = c(1, 35, 35) # bad health
```

The posterior samples of the model parameters are stored in mod_csim:

```{r}
head(mod_csim)
```

First, we’ll compute the linear part of the predictor:
```{r}
loglam1 = mod_csim[,"int"] + mod_csim[,c(2,1,3)] %*% x1
loglam2 = mod_csim[,"int"] + mod_csim[,c(2,1,3)] %*% x2
```

Next we’ll apply the inverse link:

```{r}
lam1 = exp(loglam1)
lam2 = exp(loglam2)
```

The final step is to use these samples for the $\lambda$ parameter for each individual and simulate actual number of doctor visits using the likelihood:

```{r}
(n_sim = length(lam1))
```

```{r}
y1 = rpois(n=n_sim, lambda=lam1)
y2 = rpois(n=n_sim, lambda=lam2)

plot(table(factor(y1, levels=0:18))/n_sim, pch=2, ylab="posterior prob.", xlab="visits")
points(table(y2+0.1)/n_sim, col="red")
```

Finally, we can answer the original question: What is the probability that the person with poor health will have more doctor visits than the person with good health?

```{r}
mean(y2 > y1)
```

Because we used our posterior samples for the model parameters in our simulation, this posterior predictive distribution on the number of visits for these two new individuals naturally takes into account our uncertainty in the model estimates. This is a more honest/realistic distribution than we would get if we had fixed the model parameters at their MLE or posterior means and simulated data for the new individuals.

\newpage

# Multi-level models 

\newpage

## Hierarchical models

### Data

Let’s fit our hierarhical model for counts of chocolate chips. The data can be found in 

```{r,echo=F}
dat = read.table(file="Data/cookies.dat", header=TRUE)
kable(head(dat,10), caption = "First 10 values" ) 
```

```{r,echo=F}
h<-   as.matrix(table(dat$location))
kable(t(h),caption =  "number of cookies per location")
```

We can also visualize the distribution of chips by location.

```{r,echo=F}
hist(dat$chips, main = "Histogram of  chocolate chips in total")
```

```{r,echo=F}
boxplot(chips ~ location, data=dat,main="Boxplot of Cookie production")
```


### Prior predictive checks

Before implementing the model, we need to select prior distributions for $\alpha$ and $\beta$, the hyperparameters governing the gamma distribution for the $\lambda$ parameters. First, think about what the $\lambda$’s represent. For location j, $\lambda_j$ is the expected number of chocolate chips per cookie. Hence, $\alpha$ and $\beta$ control the distribution of these means between locations. The mean of this gamma distribution will represent the overall mean of number of chips for all cookies. The variance of this gamma distribution controls the variability between locations. If this is high, the mean number of chips will vary widely from location to location. If it is small, the mean number of chips will be nearly the same from location to location.

To see the effects of different priors on the distribution of $\lambda$’s, we can simulate. Suppose we try independent exponential priors for $\alpha$ and $\beta$.

```{r,echo=F}
set.seed(112)
n_sim = 500
alpha_pri = rexp(n_sim, rate=1.0/2.0)
beta_pri = rexp(n_sim, rate=5.0)
mu_pri = alpha_pri/beta_pri
sig_pri = sqrt(alpha_pri/beta_pri^2)

summary(mu_pri)
```

```{r,echo=F}
summary(sig_pri)
```

After simulating from the priors for $\alpha$ and $\beta$, we can use those samples to simulate further down the hierarchy:

```{r,echo=F}
lam_pri = rgamma(n=n_sim, shape=alpha_pri, rate=beta_pri)
summary(lam_pri)
```

Or for a prior predictive reconstruction of the original data set:

```{r,echo=F, comment="" }
(lam_pri = rgamma(n=5, shape=alpha_pri[1:5], rate=beta_pri[1:5]))
```

```{r,echo=F, comment="" }
(y_pri = rpois(n=150, lambda=rep(lam_pri, each=30)))
```

Because these priors have high variance and are somewhat noninformative, they produce unrealistic predictive distributions. Still, enough data would overwhelm the prior, resulting in useful posterior distributions. Alternatively, we could tweak and simulate from these prior distributions until they adequately represent our prior beliefs. Yet another approach would be to re-parameterize the gamma prior, which we’ll demonstrate as we fit the model.


```{r,echo=F, message=FALSE, warning=FALSE}
library("rjags")

mod_string = " model {
for (i in 1:length(chips)) {
  chips[i] ~ dpois(lam[location[i]])
}

for (j in 1:max(location)) {
  lam[j] ~ dgamma(alpha, beta)
}

mu ~ dgamma(2.0, 1.0/5.0) ###Gamma mean = a/b
sig ~ dexp(1.0)           ### Gamma sig = a/b^2 


## we can solve the above to get 

alpha = mu^2 / sig^2 
beta = mu / sig^2   


} "
```


```{r,echo=F, message=FALSE, comment=""}
set.seed(113)

data_jags = as.list(dat)

params = c("lam", "mu", "sig")

mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)

mod_sim = coda.samples(model=mod,
                       variable.names=params,
                       n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))



par(mar = rep(3, 4))
## convergence diagnostics
plot(mod_sim)

gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)

## compute DIC
dic = dic.samples(mod, n.iter=1e3)
```

### Model checking

After assessing convergence, we can check the fit via residuals. With a hierarhcical model, there are now two levels of residuals: the observation level and the location mean level. To simplify, we’ll look at the residuals associated with the posterior means of the parameters.

First, we have observation residuals, based on the estimates of location means.
```{r, comment=""}
## observation level residuals
(pm_params = colMeans(mod_csim))
```

```{r, comment=""}
yhat = rep(pm_params[1:5], each=30)
resid = dat$chips - yhat
plot(resid)
```

```{r, comment=""}
plot(jitter(yhat), resid)
```

```{r, comment=""}
var(resid[yhat<7])
```

```{r, comment=""}
var(resid[yhat>11])
```

```{r, comment=""}
## location level residuals
lam_resid = pm_params[1:5] - pm_params["mu"]
plot(lam_resid)
abline(h=0, lty=2)
```

We don’t see any obvious violations of our model assumptions.

### Results

```{r,comment=""}
summary(mod_sim)
```

### Random intercept linear model

We can extend the linear model for the Leinhardt data on infant mortality by incorporating the region variable. We’ll do this with a hierarhcical model, where each region has its own intercept.

```{r,echo=FALSE,comment=""}
library("car")
data("Leinhardt")

str(Leinhardt)

pairs(Leinhardt)

head(Leinhardt)
```


Previously, we worked with infant mortality and income on the logarithmic scale. Recall also that we had to remove some missing data.

```{r,echo=FALSE,comment=""}
dat = na.omit(Leinhardt)
dat$logincome = log(dat$income)
dat$loginfant = log(dat$infant)
str(dat)
```

Now we can fit the proposed model:


```{r,echo=FALSE, warning=FALSE, message=FALSE, comment=""}
library("rjags")

mod_string = " model {
  for (i in 1:length(y)) {
    y[i] ~ dnorm(mu[i], prec)
    mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]
  }
  
  for (j in 1:max(region)) {
    a[j] ~ dnorm(a0, prec_a)
  }
  
  a0 ~ dnorm(0.0, 1.0/1.0e6)
  prec_a ~ dgamma(1/2.0, 1*10.0/2.0)
  tau = sqrt( 1.0 / prec_a )
  
  for (j in 1:2) {
    b[j] ~ dnorm(0.0, 1.0/1.0e6)
  }
  
  prec ~ dgamma(5/2.0, 5*10.0/2.0)
  sig = sqrt( 1.0 / prec )
} "

set.seed(116)
data_jags = list(y=dat$loginfant, log_income=dat$logincome,
                  is_oil=as.numeric(dat$oil=="yes"), region=as.numeric(dat$region))
data_jags$is_oil
table(data_jags$is_oil, data_jags$region)

params = c("a0", "a", "b", "sig", "tau")

mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3) # burn-in

mod_sim = coda.samples(model=mod,
                       variable.names=params,
                       n.iter=5e3)

mod_csim = as.mcmc(do.call(rbind, mod_sim)) # combine multiple chains

par(mar = rep(3, 4))
## convergence diagnostics
plot(mod_sim)

gelman.diag(mod_sim)
autocorr.diag(mod_sim)
autocorr.plot(mod_sim)
effectiveSize(mod_sim)
```

\newpage


## Meta analysis

\newpage


# Prior Sensitivity Analysis

When communicating results from any analysis, a responsible statistician will report and justify modeling decisions, especially assumptions. In a Bayesian analysis, there is another assumption that is open to scrutiny: the choices of prior distributions. In the models considered so far in this course, there are an infinite number of prior distributions we could have chosen from.

How do you justify the priors you choose? If they truly represent your beliefs about the parameters before analysis and the model is appropriate, then the posterior distribution truly represents your updated beliefs. If you don’t have any strong beliefs beforehand, there are often default, reference, or noninformative prior options, and you will have to select one. However, a collaborator or a boss (indeed, somebody somewhere) may not agree with your choice of prior. One way to increase the credibility of your results is to repeat the analysis under a variety of priors, and report how the results differ as a result. This process is called prior sensitivity analyis.

At a minimum you should always report your choice of model and prior. If you include a sensitivity analysis, select one or more alternative priors and describe how the results of the analysis change. If they are sensitive to the choice of prior, you will likely have to explain both sets of results, or at least explain why you favor one prior over another. If the results are not sensitive to the choice of prior, this is evidence that the data are strongly driving the results. It suggests that different investigators coming from different backgrounds should come to the same conclusions.

If the purpose of your analysis is to establish a hypothesis, it is often prudent to include a ``skeptical" prior which does not favor the hypothesis. Then, if the posterior distribution still favors the hypothesis despite the unfavorable prior, you will be able to say that the data substantially favor the hypothesis. This is the approach we will take in the following example, continued from the previous lesson.

## Example

Let’s return to the example of number of doctor visits (Poisson regression). We concluded from our previous analysis of these data that both bad health and increased age are associated with more visits. Suppose the burden of proof that bad health is actually associated with more visits rests with us, and we need to convince a skeptic.

First, let’s re-run the original analysis and remind ourselves of the posterior distribution for the badh (bad health) indicator.

```{r}
library("COUNT")
library("rjags")
```


```{r}
data("badhealth")

mod_string = " model {
    for (i in 1:length(numvisit)) {
        numvisit[i] ~ dpois(lam[i])
        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]
    }
    
    int ~ dnorm(0.0, 1.0/1e6)
    b_badh ~ dnorm(0.0, 1.0/1e4)
    b_age ~ dnorm(0.0, 1.0/1e4)
    b_intx ~ dnorm(0.0, 1.0/1e4)
} "

set.seed(102)

data_jags = as.list(badhealth)

params = c("int", "b_badh", "b_age", "b_intx")

mod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)
update(mod, 1e3)

mod_sim = coda.samples(model=mod,
                        variable.names=params,
                        n.iter=5e3)
mod_csim = as.mcmc(do.call(rbind, mod_sim))
plot(density(mod_csim[,"b_badh"]), main = "Density plot for bad health coefficient" )
```

Essentially all of the posterior probability mass is above 0, suggesting that this coefficient is positive (and consequently that bad health is associated with more visits). We obtained this result using a relatively noninformative prior. What if we use a prior that strongly favors values near 0? Let’s repeat the analysis with a normal prior on the badh coefficient that has mean 0 and standard deviation 0.2, so that the prior probability that the coefficient is less than 0.6 is >0.998>0.998. We’ll also use a small variance on the prior for the interaction term involving badh (standard deviation 0.01 because this coefficient is on a much smaller scale).



```{r}
mod2_string = " model {
    for (i in 1:length(numvisit)) {
        numvisit[i] ~ dpois(lam[i])
        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]
    }
    
    int ~ dnorm(0.0, 1.0/1e6)
    b_badh ~ dnorm(0.0, 1.0/0.2^2)
    b_age ~ dnorm(0.0, 1.0/1e4)
    b_intx ~ dnorm(0.0, 1.0/0.01^2)
} "

mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)
update(mod2, 1e3)

mod2_sim = coda.samples(model=mod2,
                        variable.names=params,
                        n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))
```

How did the posterior distribution for the coefficient of badh change?

```{r}
curve(dnorm(x, mean=0.0, sd=sqrt(1e4)), from=-3.0, to=3.0, ylim=c(0.0, 3.0), lty=2,
      main="b_badh", ylab="density", xlab="b_badh")
curve(dnorm(x, mean=0.0, sd=0.2), from=-3.0, to=3.0, col="red", lty=2, add=TRUE)
lines(density(mod_csim[,"b_badh"]))
lines(density(mod2_csim[,"b_badh"]), col="red")
legend("topleft", legend=c("noninformative prior", "posterior", "skeptical prior", "posterior"),
       lty=c(2,1,2,1), col=rep(c("black", "red"), each=2), bty="n")
```

Under the skeptical prior, our posterior distribution for b_badh has significantly dropped to between about 0.6 and 1.1. Although the strong prior influenced our inference on the magnitude of the bad health effect on visits, it did not change the fact that the coefficient is significantly above 0. In other words: even under the skeptical prior, bad health is associated with more visits, with posterior probability near 1.

We should also check the effect of our skeptical prior on the interaction term involving both age and health.


```{r}
curve(dnorm(x, mean=0.0, sd=sqrt(1e4)), from=-0.05, to=0.05, ylim=c(0.0, 140.0), lty=2,
      main="b_intx", ylab="density", xlab="b_intx")
curve(dnorm(x, mean=0.0, sd=0.01), from=-0.05, to=0.05, col="red", lty=2, add=TRUE)
lines(density(mod_csim[,"b_intx"]))
lines(density(mod2_csim[,"b_intx"]), col="red")
legend("topleft", legend=c("noninformative prior", "posterior", "skeptical prior", "posterior"),
       lty=c(2,1,2,1), col=rep(c("black", "red"), each=2), bty="n")
```

```{r}
mean(mod2_csim[,"b_intx"] > 0) # posterior probability that b_intx is positive
```

The result here is interesting. Our estimate for the interaction coefficient has gone from negative under the noninformative prior to positive under the skeptical prior, so the result is sensitive. In this case, because the skeptical prior shrinks away much of the bad health main effect, it is likely that this interaction effect attempts to restore some of the positive effect of bad health on visits. Thus, despite some observed prior sensitivity, our conclusion that bad health positively associates with more visits remains unchanged.



\newpage

# Bibliography